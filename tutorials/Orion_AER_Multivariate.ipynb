{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/Users/18235320/IdeaProjects/coloner/Orion/data/app.csv\")\n",
    "\n",
    "\n",
    "## Timestamp as time\n",
    "import numpy as np\n",
    "timestamps = pd.to_datetime(data['time'])\n",
    "timestamps = timestamps.values.astype(np.int64)\n",
    "data['timestamp'] = timestamps\n",
    "\n",
    "\n",
    "data.drop(columns=['time'], inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data by services\n",
    "app = 'rsc-cache'\n",
    "# columns['timestamp', 'throughput', 'latency', 'not_ready_pods', 'system_error', 'business_error']\n",
    "filtered_data = data[data['app'] == app]\n",
    "filtered_data.sort_values(by='timestamp', inplace=True)\n",
    "filtered_data.drop(columns=['app', 'timestamp'], inplace=True)\n",
    "\n",
    "filtered_data = filtered_data.reset_index().rename(columns={'index': 'timestamp'})\n",
    "filtered_data = filtered_data[['timestamp', 'throughput', 'system_error', 'latency', 'business_error', 'not_ready_pods']]\n",
    "train = filtered_data[20:800]\n",
    "test = filtered_data[800:]\n",
    "\n",
    "# In case univariate\n",
    "train = train.iloc[:,[0,1]]\n",
    "test = test.iloc[:,[0,1]]\n",
    "print(f\"Train data for service {app}\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data = [[-10], [1], [2], [4]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(scaler.fit(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.78571429],\n",
       "       [0.85714286],\n",
       "       [7.85714286]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [[-10], [1], [2], [100]]\n",
    "scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['timestamp', 'throughput', 'system_error', 'latency',\n",
       "       'business_error'], dtype=object)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filtered_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 6)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(orion._mlpipeline.to_dict())\n",
    "\n",
    "# train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = {\n",
    "    \"primitives\": [\n",
    "        \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate\",\n",
    "        \"sklearn.impute.SimpleImputer\",\n",
    "        \"sklearn.preprocessing.MinMaxScaler\",\n",
    "        \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences\",\n",
    "        \"orion.primitives.timeseries_preprocessing.slice_array_by_dims\",\n",
    "        \"orion.primitives.aer.AER\",\n",
    "        \"orion.primitives.aer.score_anomalies\",\n",
    "        \"orion.primitives.timeseries_anomalies.find_anomalies\"\n",
    "    ],\n",
    "    \"init_params\": {\n",
    "        \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            \"time_column\": \"timestamp\",\n",
    "            \"interval\": 1,\n",
    "            \"method\": \"mean\"\n",
    "        },\n",
    "        \"sklearn.preprocessing.MinMaxScaler#1\": {\n",
    "            \"feature_range\": [\n",
    "                -1,\n",
    "                1\n",
    "            ]\n",
    "        },\n",
    "        \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n",
    "            \"target_column\": 0,\n",
    "            \"window_size\": 5,\n",
    "            \"target_size\": 1\n",
    "        },\n",
    "        \"orion.primitives.timeseries_preprocessing.slice_array_by_dims\": {\n",
    "            \"target_index\": 0,\n",
    "            \"axis\": 2\n",
    "        },\n",
    "        \"orion.primitives.aer.AER#1\": {\n",
    "            \"epochs\": 10\n",
    "        },\n",
    "        \"orion.primitives.aer.score_anomalies#1\": {\n",
    "            \"rec_error_type\": \"dtw\",\n",
    "            \"comb\": \"mult\",\n",
    "            \"mask\": True,\n",
    "            \"lambda_rec\": 0.5\n",
    "        },\n",
    "        \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n",
    "            \"window_size_portion\": 0.33,\n",
    "            \"window_step_size_portion\": 0.1,\n",
    "            \"fixed_threshold\": True\n",
    "        }\n",
    "    },\n",
    "    \"input_names\": {\n",
    "        \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n",
    "            \"index\": \"index\"\n",
    "        }\n",
    "    },\n",
    "    \"output_names\": {\n",
    "        \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n",
    "            \"index\": \"X_index\",\n",
    "            \"target_index\": \"y_index\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "for app in data['app'].unique():\n",
    "    try:\n",
    "        # app = 'rsc-cache'\n",
    "        # Data by services\n",
    "        # columns['timestamp', 'throughput', 'latency', 'not_ready_pods', 'system_error', 'business_error']\n",
    "        filtered_data = data[data['app'] == app]\n",
    "        filtered_data.sort_values(by='timestamp', inplace=True)\n",
    "        filtered_data.drop(columns=['app', 'timestamp'], inplace=True)\n",
    "        \n",
    "        filtered_data = filtered_data.reset_index().rename(columns={'index': 'timestamp'})\n",
    "        filtered_data = filtered_data[['timestamp', 'system_error', 'business_error','latency', 'throughput',  'not_ready_pods']]\n",
    "        train = filtered_data\n",
    "        test = filtered_data[800:]\n",
    "        \n",
    "        # In case univariate\n",
    "        # train = train.iloc[:,[0,1]]\n",
    "        # test = test.iloc[:,[0,1]]\n",
    "        print(f\"Train data for service {app}\")\n",
    "        print(train.head())\n",
    "        print(train.shape)\n",
    "        feature_count = train.shape[1] - 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        from orion import Orion\n",
    "        \n",
    "        pipeline_name='aer'\n",
    "        # pipeline_name=pipeline\n",
    "        hyperparameters = {\n",
    "            # \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n",
    "            #     \"target_column\": 0,\n",
    "            #     \"window_size\": 5,\n",
    "            #     \"target_size\": 1\n",
    "            # },\n",
    "            #  \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            \n",
    "            # anomaly_transformer\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"orion.primitives.anomaly_transformer.AnomalyTransformer#1\": {\n",
    "            #         \"epochs\": 5,\n",
    "            #         \"verbose\": True\n",
    "            # },\n",
    "            \n",
    "            # vae\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"orion.primitives.vae.VAE#1\": {\n",
    "            #         \"epochs\": 5,\n",
    "            #         \"verbose\": True\n",
    "            # },\n",
    "            \n",
    "            # tadgan\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"orion.primitives.tadgan.TadGAN#1\": {\n",
    "            #     \"epochs\": 5,\n",
    "            #     \"verbose\": True\n",
    "            # },\n",
    "        \n",
    "            # matrixprofile\n",
    "            # univariate\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # }\n",
    "        \n",
    "            # # lstm_dynamic_threshold\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"keras.Sequential.LSTMTimeSeriesRegressor#1\": {\n",
    "            #     \"epochs\": 5,\n",
    "            #     \"verbose\": True\n",
    "            # }\n",
    "        \n",
    "        \n",
    "            # # lstm_autoencoder\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"keras.Sequential.LSTMSeq2Seq#1\": {\n",
    "            #         \"epochs\": 5,\n",
    "            #         \"verbose\": True,\n",
    "            #         \"window_size\": 100,\n",
    "            #         \"input_shape\": [100, feature_count],\n",
    "            #         \"target_shape\": [100, 1]\n",
    "            #     },\n",
    "        \n",
    "            ## GOOD for latency\n",
    "            # dense_autoencoder\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # },\n",
    "            # \"keras.Sequential.DenseSeq2Seq#1\": {\n",
    "            #     \"epochs\": 5,\n",
    "            #     \"verbose\": True,\n",
    "            #     \"window_size\": 100,\n",
    "            #     \"input_shape\": [100, feature_count],\n",
    "            #     \"target_shape\": [100, 1]\n",
    "            # },\n",
    "           \n",
    "            # arima\n",
    "            # univariate\n",
    "            # \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "            #     'interval': 1\n",
    "            # }\n",
    "           \n",
    "            \n",
    "            # aer\n",
    "            \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "                'interval': 1\n",
    "            },\n",
    "\n",
    "            'orion.primitives.aer.AER#1': {\n",
    "                'epochs': 5,\n",
    "                'verbose': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        orion = Orion(\n",
    "            pipeline=pipeline_name,\n",
    "            hyperparameters=hyperparameters\n",
    "        )\n",
    "        print(orion)\n",
    "        \n",
    "        \n",
    "        orion.fit(train)\n",
    "        \n",
    "        df = test\n",
    "        anomalies = orion.detect(df)\n",
    "        print(anomalies.head())\n",
    "        columns = df.columns.values[1:]\n",
    "        size = columns.shape[0]\n",
    "        print(size)\n",
    "        if size == 1:\n",
    "            axes = [plt]\n",
    "            print(axes)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(nrows=size, ncols = 1, figsize= (12,8), sharex=True)\n",
    "        for ax, feature in zip(axes, columns):\n",
    "            ax.plot(df['timestamp'], df[feature], label=feature, marker='o')\n",
    "            for _, row in anomalies.iterrows():\n",
    "                ax.axvspan(row['start'], row['end'], color = 'red', alpha = min(1,row['severity']))\n",
    "            if size != 1:\n",
    "                ax.set_title(f\"{app} feature: {feature}\")\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(app)\n",
    "        if size != 1:\n",
    "            plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        df = train\n",
    "        anomalies = orion.detect(df)\n",
    "        print(anomalies.head())\n",
    "        columns = df.columns.values[1:]\n",
    "        columns.shape[0]\n",
    "        \n",
    "        if size == 1:\n",
    "            axes = [plt]\n",
    "            print(axes)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(nrows=size, ncols = 1, figsize= (12,8), sharex=True)\n",
    "        for ax, feature in zip(axes, columns):\n",
    "            ax.plot(df['timestamp'], df[feature], label=feature, marker='o')\n",
    "            for _, row in anomalies.iterrows():\n",
    "                ax.axvspan(row['start'], row['end'], color = 'red', alpha = min(1,row['severity']))\n",
    "            if size != 1:\n",
    "                ax.set_title(f\"Train Feature: {feature}\")\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            \n",
    "        plt.xticks(rotation=45)\n",
    "        if size != 1:\n",
    "            plt.tight_layout()\n",
    "        plt.show()\n",
    "        # break\n",
    "    except Exception:\n",
    "        print(f\"Cannot find anomalies for service { app }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be a ``pandas.DataFrame`` containing a table with the detected anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "anomalies = orion.detect(df)\n",
    "print(anomalies.head())\n",
    "columns = df.columns.values[1:]\n",
    "columns.shape[0]\n",
    "\n",
    "if size == 1:\n",
    "    axes = [plt]\n",
    "    print(axes)\n",
    "else:\n",
    "    fig, axes = plt.subplots(nrows=size, ncols = 1, figsize= (12,8), sharex=True)\n",
    "for ax, feature in zip(axes, columns):\n",
    "    ax.plot(df['timestamp'], df[feature], label=feature, marker='o')\n",
    "    for _, row in anomalies.iterrows():\n",
    "        ax.axvspan(row['start'], row['end'], color = 'red', alpha = min(1,row['severity']))\n",
    "    if size != 1:\n",
    "        ax.set_title(f\"Feature: {feature}\")\n",
    "        ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "plt.xticks(rotation=45)\n",
    "if size != 1:\n",
    "    plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data['time'] = pd.to_datetime(filtered_data['timestamp'], unit='ns')\n",
    "to_plot = filtered_data.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.heatmap(filtered_data.set_index('timestamp').corr(), annot=True, cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['throughput', 'latency', 'not_ready_pods', 'system_error',\n",
       "       'business_error', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "current_start = None\n",
    "result = []\n",
    "for i in range(len(labels)):\n",
    "    if current_start != None and labels.iloc[i]['label'] == 0:\n",
    "        result.append({\"start\": current_start, \"end\" : labels.iloc[i-1][\"timestamp\"]})\n",
    "        current_start = None\n",
    "    elif current_start == None and labels.iloc[i]['label'] == 1:\n",
    "        current_start = row['timestamp']\n",
    "if current_start != None:\n",
    "    result.append({\"start\": current_start, \"end\" : labels.iloc[-1][\"timestamp\"]})\n",
    "\n",
    "df_range = pd.DataFrame(result)\n",
    "print(df_range)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter rows where label is 1\n",
    "df_filtered = labels[labels['label'] == 1]\n",
    "\n",
    "# Identify consecutive groups\n",
    "df_filtered['group'] = (df_filtered['timestamp'].diff() != 1).cumsum()\n",
    "\n",
    "# Aggregate start and end of each group\n",
    "df_range2 = df_filtered.groupby('group').agg(start=('timestamp', 'first'), end=('timestamp', 'last')).reset_index(drop=True)\n",
    "\n",
    "print(df_range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['timestamp'].diff() != 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['timestamp'] = labels['timestamp'].view('int64') \n",
    "labels['timestamp'] /= 1000000000\n",
    "labels['timestamp'] = labels['timestamp'].astype('int64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion-env",
   "language": "python",
   "name": "orion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
